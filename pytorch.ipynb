{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch is a popular deep learning framework and it's easy to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we read the mnist data, preprocess them and encapsulate them into dataloader form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./mnist/MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72023466fc8d45e69dc6b44a09147d2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/MNIST\\raw\\train-images-idx3-ubyte.gz to ./mnist/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./mnist/MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a64adce1ec93435e9082f183e5bdb46a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/MNIST\\raw\\train-labels-idx1-ubyte.gz to ./mnist/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./mnist/MNIST\\raw\\t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 503: Service Unavailable\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./mnist/MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "927bda944a3444d3a6676685648cf47c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./mnist/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./mnist/MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c955e527a9b44e8a4e013cdefbbc05e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./mnist/MNIST\\raw\n",
      "\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aaa\\anaconda3\\envs\\attack_and_defense\\lib\\site-packages\\torchvision\\datasets\\mnist.py:502: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:143.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "# preprocessing\n",
    "# Normalize(mean, std)，通过image = (imgae - mean) / std来归一化\n",
    "# 一般为一个三元组如(0.5,0.5,0.5)，灰度图就一个channel所以一个值\n",
    "normalize = transforms.Normalize(mean=[.5], std=[.5])\n",
    "transform = transforms.Compose([transforms.ToTensor(), normalize])\n",
    "\n",
    "# download and load the data\n",
    "train_dataset = torchvision.datasets.MNIST(root='./mnist/', train=True, transform=transform, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./mnist/', train=False, transform=transform, download=False)\n",
    "\n",
    "# encapsulate them into dataloader form\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "test_loader = data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we define the model, object function and optimizer that we use to classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "# TODO:define model\n",
    "    def __init__(self):\n",
    "        super(SimpleNet,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.fc1 = nn.Linear(9216, 64)\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x) # (n,1,28,28)->(n,32,26,26)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x) # (n,32,26,26)->(n,64,24,24)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2) # (n,64,24,24)->(n,64,12,12)\n",
    "        x = torch.flatten(x, 1) # (n,64,12,12)->(n,9216)\n",
    "        x = self.fc1(x) # (n,9216)->(n,64)\n",
    "        x = self.fc2(x) # (n,64)->(n,10)\n",
    "        out = F.softmax(x, dim=1)\n",
    "        return out\n",
    "    \n",
    "model = SimpleNet().to(device)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# TODO:define loss function and optimiter\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can start to train and evaluate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 468/468 [00:07<00:00, 60.18it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:01<00:00, 63.58it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 468/468 [00:07<00:00, 60.63it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:01<00:00, 67.13it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 468/468 [00:08<00:00, 57.66it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:01<00:00, 55.23it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 468/468 [00:08<00:00, 56.56it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:01<00:00, 63.07it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 468/468 [00:07<00:00, 59.49it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:01<00:00, 59.52it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 468/468 [00:07<00:00, 60.46it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:01<00:00, 59.88it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 468/468 [00:07<00:00, 58.58it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:01<00:00, 65.77it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 468/468 [00:08<00:00, 56.34it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:01<00:00, 63.79it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 468/468 [00:07<00:00, 59.54it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:01<00:00, 61.77it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 468/468 [00:08<00:00, 55.62it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:01<00:00, 60.02it/s]\n"
     ]
    }
   ],
   "source": [
    "# train and evaluate\n",
    "\n",
    "data_num = 0\n",
    "\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    corrects = 0\n",
    "    for images, labels in tqdm(train_loader):\n",
    "        # TODO:forward + backward + optimize\n",
    "        x, y = images.to(device), labels.to(device)\n",
    "        data_num += x.shape[0]\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        model.train()\n",
    "        output = model(x)\n",
    "        \n",
    "        loss = criterion(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            pred = torch.argmax(output, dim=1)\n",
    "            corrects += (pred==y).float().sum()   # 这个.sum()很重要，要不然train_accuracy是一个(10,8,64)的玩意，我们期待它是(10,)\n",
    "    train_acc = corrects / float(data_num)\n",
    "    train_accuracy.append(train_acc.float())\n",
    "    \n",
    "    corrects = 0\n",
    "    data_num = 0\n",
    "    # evaluate\n",
    "    # TODO:calculate the accuracy using traning and testing dataset\n",
    "    for images, labels in tqdm(test_loader):\n",
    "        x_test, y_test = images.to(device), labels.to(device)\n",
    "        data_num += x.shape[0]\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = model(x)\n",
    "        pred = output.argmax(dim=1)\n",
    "        corrects += (pred==y).float().sum()\n",
    "    test_acc = corrects / float(data_num)\n",
    "    test_accuracy.append(test_acc.float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q5:\n",
    "Please print the training and testing accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: ['98.90%', '84.76%', '84.83%', '84.81%', '84.86%', '84.77%', '84.84%', '84.81%', '84.89%', '84.85%']\n",
      "Testing accuracy: ['100.00%', '98.44%', '100.00%', '100.00%', '99.22%', '98.44%', '100.00%', '97.66%', '100.00%', '99.22%']\n"
     ]
    }
   ],
   "source": [
    "print('Training accuracy:', ['%0.2f%%' % (float(100*x)) for x in train_accuracy])\n",
    "print('Testing accuracy:',['%0.2f%%' % (float(100*x)) for x in test_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:attack_and_defense]",
   "language": "python",
   "name": "conda-env-attack_and_defense-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
